{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180acc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e1f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    auto_offset_reset='earliest',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('ascii','ignore'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4875b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer.subscribe('jaeger-spans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07fbcfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from json import JSONEncoder\n",
    "import numpy\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, numpy.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53906fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vmadmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# open the file in the read mode\n",
    "f = open('final_log_data.csv', 'r')\n",
    "\n",
    "# create the csv reader\n",
    "csv_reader = csv.reader(f)\n",
    "next(csv_reader, None)  # skip the headers\n",
    "\n",
    "num_of_comments = 1000\n",
    "i = 1\n",
    "comments = []\n",
    "durations = []\n",
    "labels = []\n",
    "for row in csv_reader:\n",
    "    comment = row[2]\n",
    "    duration = row[0]\n",
    "    label = row[3]\n",
    "\n",
    "    \n",
    "    comment = re.sub(r'\\n', '', comment)\n",
    "    comment = re.sub(r'<br />', '', comment)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    comment = comment.lower()\n",
    "    \n",
    "    comment = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?', '', comment)\n",
    "    comment = re.sub(r'\\d+', '', comment)\n",
    "    \n",
    "    # remove stop words\n",
    "    comment = ' '.join([word for word in comment.split() if word not in (stop)])\n",
    "    \n",
    "    comments.append(comment)\n",
    "    durations.append(duration)\n",
    "    labels.append(label)\n",
    "\n",
    "    i = i + 1\n",
    "    if i > num_of_comments:\n",
    "        break\n",
    "\n",
    "f.close()\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(np.array(comments))\n",
    "\n",
    "comments_seq = token.texts_to_sequences(np.array(comments))\n",
    "comments_pad_seq = pad_sequences(comments_seq, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2022-02-02 18:08:27 0.888699900 In my opinion this is Marvel\\'s most epic and darkest entry in it\\'s long line of superhero movies. From its brilliantly choreographed, heart-pounding action scenes to it\\'s hilarious character interactions Avengers infinity war will satisfy any superhero enthusiast. The part that really stood out for mw is how Russo Brothers portrayed Thanos - a menacing but genius villian with movie that eaves us philosophying about our existense. A must watch! 0\n",
      "This looks safe\n",
      "2 2022-02-02 18:08:27 0.403700 select * from movie_db; 1\n",
      "This can be Anomolous\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "\n",
    "classifier = keras.models.load_model('lstm_model.h5')\n",
    "\n",
    "for message in consumer:\n",
    "    # message value and key are raw bytes -- decode if necessary!\n",
    "    # e.g., for unicode: `message.value.decode('utf-8')`\n",
    "    # print (\"%s:%d:%d: key=%s value=%s\" % (message.topic, message.partition, message.offset, message.key, message.value))\n",
    "    \n",
    "    if message.value['process']['serviceName'] == 'frontend':\n",
    "        \n",
    "        # open the file in the append mode\n",
    "        f1 = open('log_prediction.csv', 'a', encoding='utf-8')\n",
    "        # create the csv writer\n",
    "        writer = csv.writer(f1)\n",
    "                \n",
    "        http_method = re.findall(r\"{'key': 'http\\.method', 'vStr': '(.+?)'}\",str(message.value))\n",
    "        http_target = re.findall(r\"{'key': 'http\\.target', 'vStr': '(.+?)'}\",str(message.value))\n",
    "        http_status_code = re.findall(r\"{'key': 'http\\.status_code', 'vType': '.*?', 'vInt64': '(\\d+)'}\",str(message.value))\n",
    "        http_url = re.findall(r\"{'key': 'http\\.url', 'vStr': '(.+?)'}\",str(message.value))\n",
    "        duration = re.findall(r\"(.+?)s\",str(message.value['duration']))\n",
    "        \n",
    "        if len(http_method) > 0:\n",
    "            http_method = http_method[0]\n",
    "        if len(http_target) > 0:\n",
    "            http_target = http_target[0]\n",
    "        if len(http_url) > 0:\n",
    "            http_url = http_url[0]\n",
    "        if len(http_status_code) > 0:\n",
    "            http_status_code = http_status_code[0]\n",
    "        if len(duration) > 0:\n",
    "            duration = duration[0]\n",
    "        \n",
    "        if http_method == 'POST':\n",
    "            if \"comment\" in http_url:\n",
    "                comment = \"\"\n",
    "                fields = message.value['logs'][0]['fields']\n",
    "                description1 = re.findall(r\"{'key': 'body', 'vStr': '(.+?)'}\",str(fields))\n",
    "                if len(description1) > 0:\n",
    "                    temp = re.findall(r'\"description\":\"(.+?)\",\"_links\":',str(description1[0]))\n",
    "                    user_id = re.findall(r'\"userId\":(.+?),\"movieId\":',str(description1[0]))\n",
    "                    movie_id = re.findall(r'\"movieId\":(.+?),\"description\":',str(description1[0]))\n",
    "                    \n",
    "                    if len(temp) > 0:\n",
    "                        comment = temp[0]\n",
    "                        user_id = user_id[0]\n",
    "                        movie_id = movie_id[0]\n",
    "                \n",
    "                \n",
    "                data = [duration,comment]\n",
    "\n",
    "\n",
    "                # PREDICTION\n",
    "\n",
    "                duration = data[0]\n",
    "                comment_orig = data[1]\n",
    "\n",
    "\n",
    "                comment = re.sub(r'\\n', '', comment_orig)\n",
    "                comment = re.sub(r'<br />', '', comment)\n",
    "\n",
    "                # Converting to Lowercase\n",
    "                comment = comment.lower()\n",
    "\n",
    "                comment = re.sub(r'(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?', '', comment)\n",
    "                comment = re.sub(r'\\d+', '', comment)\n",
    "\n",
    "                # remove stop words\n",
    "                comment = ' '.join([word for word in comment.split() if word not in (stop)])\n",
    "                comment_seq = token.texts_to_sequences(np.array(comment).reshape(-1))\n",
    "                comment_pad_seq = pad_sequences(comment_seq, maxlen=300)\n",
    "                \n",
    "                comment_features = comment_pad_seq\n",
    "                \n",
    "\n",
    "                # A = np.array([duration], dtype=float)[:,None]\n",
    "                # data = np.concatenate((A, comment_features), axis=1)\n",
    "                data = comment_features\n",
    "                data_prob = classifier.predict(data)\n",
    "                data_pred = np.argmax(data_prob, axis=1)\n",
    "\n",
    "                date = datetime.now().strftime('%Y-%m-%d')\n",
    "                time = datetime.now().strftime('%H:%M:%S')\n",
    "                \n",
    "                numpyData = {\"array\": data,\"comment_date\": date,'comment':comment,'spanId':message.value['spanId'],'userId':user_id,'movieId':movie_id}\n",
    "                \n",
    "                print(counter, date, time, duration, comment_orig, data_pred[0])\n",
    "                \n",
    "                result = data_pred[0]\n",
    "                if result==0:\n",
    "                    print(\"This looks safe\")\n",
    "                elif result==1:\n",
    "                    print(\"This can be Anomalous\")\n",
    "                \n",
    "                log_prediction = [date,time,duration,comment_orig,data_pred[0]]\n",
    "                # write a row to the csv file\n",
    "                writer.writerow(log_prediction)\n",
    "                \n",
    "                counter = counter + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab3422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
